{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Analysis\n",
    "\n",
    "Script to explore prediction of fraud with the following dataset:\n",
    "Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi.\n",
    "Calibrating Probability with Undersampling for Unbalanced Classification.\n",
    "In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
    "\n",
    "The dataset can be downloaded from:\n",
    "https://www.kaggle.com/dalpozz/creditcardfraud\n",
    "\n",
    "The approach will be to use an autoencoder to understand learn about the dataset\n",
    "and to identify the transactions with the biggest error from the autoencoder reconstruction.\n",
    "\n",
    "The autoencoder will then be used as pre-training for a neural network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda\n",
    "import sklearn as skl\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define analysis hyper-parameters\n",
    "batch_size=64\n",
    "lr= 0.002\n",
    "no_epochs=20\n",
    "hidden_nodes=[14,7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU and CUDA libraries\n",
    "HAS_CUDA=torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to get consistant results during model development by fixing random seeds\n",
    "np.random.seed(39)\n",
    "torch.manual_seed(10)\n",
    "if HAS_CUDA:\n",
    "    torch.cuda.manual_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "dataset=pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data imported and normalized\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Process time column to catagorize into time of day quarters, and normalize the data\n",
    "from utils import add_quarter_and_normalize\n",
    "dataset = add_quarter_and_normalize(dataset)\n",
    "data = dataset.drop(['Time','qtr_num'], axis=1)\n",
    "\n",
    "print(' Data imported and normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Note this shuffles the data by default before splitting\n",
    "data_tr, data_val=train_test_split(data,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create version of data for training the Autoencoder.  For this purpose only use non-fraudulent cases and drop the \"Class\" field since it is not needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset and drop the class field\n",
    "AE_input_tr = data_tr[data_tr.Class==0]\n",
    "AE_input_tr = AE_input_tr.drop(['Class', 'Amt_To_Keep'], axis=1)\n",
    "AE_input_val = data_val.drop(['Class', 'Amt_To_Keep'], axis=1)\n",
    "\n",
    "# Convert arrays to PyTorch Tensors\n",
    "AE_input_tr=torch.Tensor.float(torch.from_numpy(AE_input_tr.values))\n",
    "AE_input_val=torch.Tensor.float(torch.from_numpy(AE_input_val.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of Autoencoder\n",
    "from AutoEncoders import AutoEncoder_Multi_Layer\n",
    "num_features=AE_input_tr.shape[1]\n",
    "AE_model=AutoEncoder_Multi_Layer(num_features, hidden_sizes=hidden_nodes)\n",
    "\n",
    "# If the computer has a GPU, move the model to the GPU\n",
    "if HAS_CUDA:\n",
    "    AE_model.cuda()\n",
    "\n",
    "# Define optimiser algorithm to use (using Adam in this case)\n",
    "import torch.optim as optim\n",
    "AE_opt=optim.Adam(AE_model.parameters(), lr=lr)\n",
    "\n",
    "# Define how to adjust learning rate\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "AE_sched = optim.lr_scheduler.ReduceLROnPlateau(AE_opt, mode='min', verbose=True, \n",
    "                                                patience=5, factor=0.2)\n",
    "\n",
    "# Define cost function to use\n",
    "AE_loss=nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model manager instance to control training and validation\n",
    "from nnModels import NN_Manage\n",
    "AE_manager=NN_Manage(model = AE_model,\n",
    "                     loss_function = AE_loss, \n",
    "                     optimizer = AE_opt, \n",
    "                     scheduler=AE_sched,\n",
    "                     HAS_CUDA=HAS_CUDA, \n",
    "                     my_logger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  Batch: 0 Loss: 0.229753\n",
      "Train Epoch: 0  Batch: 100 Loss: 0.519741\n",
      "Train Epoch: 0  Batch: 200 Loss: 0.270257\n",
      "Train Epoch: 0  Batch: 300 Loss: 0.254409\n",
      "Train Epoch: 0  Batch: 400 Loss: 0.201296\n",
      "Train Epoch: 0  Batch: 500 Loss: 0.305808\n",
      "Train Epoch: 0  Batch: 600 Loss: 0.224236\n",
      "Train Epoch: 0  Batch: 700 Loss: 0.237990\n",
      "Train Epoch: 0  Batch: 800 Loss: 0.275711\n",
      "Train Epoch: 0  Batch: 900 Loss: 0.222909\n",
      "Train Epoch: 0  Batch: 1000 Loss: 0.260364\n",
      "Train Epoch: 0  Batch: 1100 Loss: 0.247218\n",
      "Train Epoch: 0  Batch: 1200 Loss: 0.223768\n",
      "Train Epoch: 0  Batch: 1300 Loss: 0.203623\n",
      "Train Epoch: 0  Batch: 1400 Loss: 0.217012\n",
      "Train Epoch: 0  Batch: 1500 Loss: 0.215013\n",
      "Train Epoch: 0  Batch: 1600 Loss: 0.229555\n",
      "Train Epoch: 0  Batch: 1700 Loss: 0.549973\n",
      "Train Epoch: 0  Batch: 1800 Loss: 0.296072\n",
      "Train Epoch: 0  Batch: 1900 Loss: 0.238299\n",
      "Train Epoch: 0  Batch: 2000 Loss: 0.222937\n",
      "Train Epoch: 0  Batch: 2100 Loss: 0.277053\n",
      "Train Epoch: 0  Batch: 2200 Loss: 0.243090\n",
      "Train Epoch: 0  Batch: 2300 Loss: 0.223151\n",
      "Train Epoch: 0  Batch: 2400 Loss: 0.229681\n",
      "Train Epoch: 0  Batch: 2500 Loss: 0.220437\n",
      "Train Epoch: 0  Batch: 2600 Loss: 0.234140\n",
      "Train Epoch: 0  Batch: 2700 Loss: 0.318925\n",
      "Train Epoch: 0  Batch: 2800 Loss: 0.286930\n",
      "Train Epoch: 0  Batch: 2900 Loss: 0.238734\n",
      "Train Epoch: 0  Batch: 3000 Loss: 0.265898\n",
      "Train Epoch: 0  Batch: 3100 Loss: 0.226233\n",
      "Train Epoch: 0  Batch: 3200 Loss: 0.241430\n",
      "Train Epoch: 0  Batch: 3300 Loss: 0.261711\n",
      "Train Epoch: 0  Batch: 3400 Loss: 0.305047\n",
      "Train Epoch: 0  Batch: 3500 Loss: 0.255684\n",
      "====> Epoch: 0 Average training loss:      0.2634\n",
      "====> Epoch: 0 Average validation loss:          0.2938\n",
      "====> Epoch: 1 Average training loss:      0.2634\n",
      "====> Epoch: 1 Average validation loss:          0.2935\n",
      "====> Epoch: 2 Average training loss:      0.2632\n",
      "====> Epoch: 2 Average validation loss:          0.2955\n",
      "====> Epoch: 3 Average training loss:      0.2633\n",
      "====> Epoch: 3 Average validation loss:          0.2939\n",
      "Epoch    23: reducing learning rate of group 0 to 4.0000e-04.\n",
      "====> Epoch: 4 Average training loss:      0.2598\n",
      "====> Epoch: 4 Average validation loss:          0.2900\n",
      "====> Epoch: 5 Average training loss:      0.2595\n",
      "====> Epoch: 5 Average validation loss:          0.2899\n",
      "====> Epoch: 6 Average training loss:      0.2594\n",
      "====> Epoch: 6 Average validation loss:          0.2901\n",
      "====> Epoch: 7 Average training loss:      0.2593\n",
      "====> Epoch: 7 Average validation loss:          0.2911\n",
      "====> Epoch: 8 Average training loss:      0.2592\n",
      "====> Epoch: 8 Average validation loss:          0.2900\n",
      "====> Epoch: 9 Average training loss:      0.2591\n",
      "====> Epoch: 9 Average validation loss:          0.2897\n",
      "====> Epoch: 10 Average training loss:      0.2590\n",
      "====> Epoch: 10 Average validation loss:          0.2901\n",
      "====> Epoch: 11 Average training loss:      0.2590\n",
      "====> Epoch: 11 Average validation loss:          0.2897\n",
      "====> Epoch: 12 Average training loss:      0.2590\n",
      "====> Epoch: 12 Average validation loss:          0.2915\n",
      "====> Epoch: 13 Average training loss:      0.2590\n",
      "====> Epoch: 13 Average validation loss:          0.2895\n",
      "====> Epoch: 14 Average training loss:      0.2589\n",
      "====> Epoch: 14 Average validation loss:          0.2898\n",
      "====> Epoch: 15 Average training loss:      0.2588\n",
      "====> Epoch: 15 Average validation loss:          0.2897\n",
      "====> Epoch: 16 Average training loss:      0.2588\n",
      "====> Epoch: 16 Average validation loss:          0.2897\n",
      "====> Epoch: 17 Average training loss:      0.2588\n",
      "====> Epoch: 17 Average validation loss:          0.2899\n",
      "====> Epoch: 18 Average training loss:      0.2588\n",
      "====> Epoch: 18 Average validation loss:          0.2901\n",
      "====> Epoch: 19 Average training loss:      0.2588\n",
      "====> Epoch: 19 Average validation loss:          0.2896\n",
      "Epoch    39: reducing learning rate of group 0 to 8.0000e-05.\n"
     ]
    }
   ],
   "source": [
    "# Train Autoencoder\n",
    "AE_test_loss=0\n",
    "for epoch in range(no_epochs):\n",
    "    # Train over training data\n",
    "    train_loss=AE_manager.train(epoch, shuffle = True, inputs = AE_input_tr,\n",
    "                                targets = AE_input_tr, batch_size = batch_size)\n",
    "    \n",
    "    # Assess performance against validation data\n",
    "    AE_test_loss=AE_manager.test(epoch,inputs = AE_input_val, targets = AE_input_val,\n",
    "                                 batch_size=batch_size)\n",
    "    \n",
    "    # Adjust learning rate if required\n",
    "    AE_manager.scheduler.step(AE_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss:       0.25875\n",
      "Validation Loss:  0.28962\n"
     ]
    }
   ],
   "source": [
    "# Check model losses for training and validation\n",
    "print()\n",
    "print('Train Loss:      {:8.5f}'.format(train_loss))\n",
    "print('Validation Loss: {:8.5f}'.format(AE_test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save case (y or ret): y\n",
      "Case name to use to save model :base_case\n",
      "Directory exists already!\n",
      "Case name to use to save model :base_case\n",
      "Autoencoder model saved\n"
     ]
    }
   ],
   "source": [
    "# Get case name to save file under\n",
    "import pickle\n",
    "import feather\n",
    "if input('Save case (y or ret): ') =='y':\n",
    "    while True:\n",
    "        case_name=input('Case name to use to save model :')\n",
    "        directory=os.path.dirname(case_name)\n",
    "        if not os.path.exists(case_name):\n",
    "            os.mkdir(\"./\"+case_name)\n",
    "            break\n",
    "        else:\n",
    "            print('Directory exists already!')\n",
    "    # Save files - model state dictionary and other key parameters\n",
    "    # Save model with weights\n",
    "    if HAS_CUDA:\n",
    "        AE_model.cpu()\n",
    "    torch.save(AE_model.state_dict(), case_name+\"/demo_state_dict_cpu.pt\")\n",
    "    # Save critical model parameters\n",
    "    with open(case_name+'/'+'analysis_params.pkl', 'wb') as f:\n",
    "        pickle.dump([batch_size, lr, no_epochs, hidden_nodes],f)\n",
    "        f.close\n",
    "\n",
    "    # Save training and validation datasets to ensure consistency with classifier\n",
    "    feather.write_dataframe(data_tr,f'{case_name}/data_tr.feather')\n",
    "    feather.write_dataframe(data_val,f'{case_name}/data_val.feather')\n",
    "\n",
    "    #data_val.to_feather(f'{case_name}/data_val')\n",
    "\n",
    "    print(\"Autoencoder model saved\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
